#!/usr/bin/env python3
"""
build.py â€“ WetterArena v9.0  (failsafe)

â€¢ respektiert GeoSphere-Rate-Limits (5 req/s, 240 req/h)
â€¢ CLI-Flags:
      --backfill YYYY-MM-DD YYYY-MM-DD
      --stations  id1,id2,â€¦     (optional)
      --skip-ok                (ignoriert fehlgeschlagene BlÃ¶cke; sonst Abbruch)
â€¢ bricht NACH ZWEI FEHLÂ­BLÃ–CKEN IN FOLGE ZWINGEND ab
â€¢ schreibt in Supabase-Postgres + site/last7.csv
"""

from __future__ import annotations
import os, sys, csv, time, random, argparse, textwrap, datetime as dt
from collections import deque
from typing import Sequence, List

import requests, psycopg2

# â”€â”€â”€â”€â”€ Konfiguration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PG_URI   = os.environ["PG_URI"]        # postgresql://â€¦
DATASET  = "klima-v2-1d"
BASE     = "https://dataset.api.hub.geosphere.at/v1"
META_CSV = "stations.csv"
SITE_DIR = "site"
TEMPLATE = "index_template.html"

CHUNK_SIZE   = 488           # max. station_ids pro Aufruf
MAX_RETRIES  = 3
MAX_WAIT     = 600           # >10 min  â†’ Abbruch
MAX_FAILS    = 2             
PARAMS       = ["tl_i","tlmax","tlmin","rr","so_h"]   # abruf + DB-Spalten
META_COLS    = ["station","name","state","lat","lon","date"]
COLS         = META_COLS + PARAMS

# GeoSphere-Limits
MAX_PER_SEC, MAX_PER_HR = 5, 220
HIST_SEC = deque(maxlen=MAX_PER_SEC)
HIST_HR  = deque(maxlen=MAX_PER_HR)
REQ_COUNT = 0

# â”€â”€â”€â”€â”€ Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs(SITE_DIR, exist_ok=True)
SESSION = requests.Session()
TIMEOUT = 120

def throttle():
    """einhÃ¤lt 5 req/s  &  240 req/h + zufÃ¤llige Jitter-Pausen."""
    global REQ_COUNT
    now = time.time()

    # 5 pro Sek.
    while HIST_SEC and now - HIST_SEC[0] >= 1: HIST_SEC.popleft()
    if len(HIST_SEC) >= MAX_PER_SEC:
        time.sleep(1 - (now - HIST_SEC[0]) + 0.05)

    # 240 pro Std.
    while HIST_HR and now - HIST_HR[0] >= 3600: HIST_HR.popleft()
    if len(HIST_HR) >= MAX_PER_HR:
        wait = 3600 - (now - HIST_HR[0]) + 1
        if wait > MAX_WAIT:
            sys.exit(f"ğŸš«  hourly cap â€“ need {wait:.0f}s (>10 min). Abort.")
        print(f"â¸  hourly cap â€“ sleep {wait:.0f}s", file=sys.stderr)
        time.sleep(wait)

    # â€menschlicheâ€œ Jitter-Pause
    time.sleep(random.uniform(0.3, 0.9))
    REQ_COUNT += 1
    if REQ_COUNT % 10 == 0:
        time.sleep(random.uniform(2.0, 4.0))

def stamp():
    t = time.time()
    HIST_SEC.append(t)
    HIST_HR.append(t)

def log(*a): print(*a, file=sys.stderr)

# â”€â”€â”€â”€â”€ Meta-Daten laden â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_stations() -> tuple[list[int], dict[int,dict]]:
    ids, meta = [], {}
    today = dt.date.today()
    with open(META_CSV, newline="", encoding="utf-8") as f:
        for r in csv.DictReader(f):
            if dt.date.fromisoformat(r["Enddatum"][:10]) < today: continue
            sid = int(r["id"]); ids.append(sid)
            meta[sid] = {"name": r["Stationsname"].strip(),
                         "state": r["Bundesland"].strip()}
    return ids, meta

ALL_IDS, META = load_stations()

# â”€â”€â”€â”€â”€ Datenabruf â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_json(day: dt.date, ids: Sequence[int]) -> dict|None:
    url = (
        f"{BASE}/station/historical/{DATASET}"
        f"?start={day}&end={day}"
        f"&station_ids={','.join(map(str,ids))}"
        f"&parameters={','.join(p.upper() for p in PARAMS)}"
    )
    for attempt in range(MAX_RETRIES + 1):
        throttle()
        try:
            r = SESSION.get(url, timeout=TIMEOUT)
            stamp()
            if r.status_code == 200:
                return r.json()

            if r.status_code == 429:
                reset = int(r.headers.get("ratelimit-reset", "30"))
                if reset > MAX_WAIT:
                    sys.exit(f"ğŸš«  API reset {reset}s (>10 min) â€“ abort.")
                log(f"â†» 429 â€“ sleep {reset}s")
                time.sleep(reset + random.uniform(1,3))
                continue            # noch ein Versuch

            # 4xx/5xx â†’ Retry-Backoff
            back = 2 ** attempt
            if attempt >= MAX_RETRIES:
                return None
            log(f"â†»  HTTP {r.status_code} â€“ retry {attempt+1} in {back}s")
            time.sleep(back + random.uniform(0.5,1.0))

        except (requests.Timeout,
                requests.ConnectionError,
                requests.HTTPError) as ex:
            back = 2 ** attempt
            if attempt >= MAX_RETRIES:
                return None
            log(f"â†»  {ex.__class__.__name__} â€“ retry {attempt+1} in {back}s")
            time.sleep(back + random.uniform(0.5,1.0))
    return None

def rows_from_json(js: dict) -> List[List]:
    if not js or not js.get("features"): return []
    dates = [ts[:10] for ts in js["timestamps"]]
    out   = []
    for feat in js["features"]:
        sid, coords = feat["properties"]["station"], feat["geometry"]["coordinates"]
        pdata = feat["properties"]["parameters"]
        meta  = META.get(sid, {})
        for i, d in enumerate(dates):
            row = [sid, meta.get("name"), meta.get("state"), coords[0], coords[1], d]
            for p in PARAMS:
                arr = pdata.get(p, {}).get("data", [])
                row.append(arr[i] if i < len(arr) else None)
            out.append(row)
    return out

SQL = textwrap.dedent(f"""
    INSERT INTO daily ({','.join(COLS)})
    VALUES ({','.join('%s' for _ in COLS)})
    ON CONFLICT (station,date) DO NOTHING
""")

def upsert(rows: List[List]) -> int:
    if not rows: return 0
    with psycopg2.connect(PG_URI) as conn, conn.cursor() as cur:
        cur.executemany(SQL, rows)
        return cur.rowcount or 0

def export_last7():
    cutoff = dt.date.today() - dt.timedelta(days=7)
    with psycopg2.connect(PG_URI) as c, c.cursor() as cur:
        cur.execute(f"SELECT {','.join(COLS)} FROM daily WHERE date >= %s ORDER BY date,station", (cutoff,))
        with open(f"{SITE_DIR}/last7.csv","w",newline="") as f:
            w = csv.writer(f); w.writerow(COLS); w.writerows(cur)

# â”€â”€â”€â”€â”€ Hauptablauf pro Tag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_day(day: dt.date, ids: list[int], skip_ok: bool) -> bool:
    """True = ok, False = komplett gescheitert (â‰¥1 Block)"""
    failed_blocks = 0
    for block in (ids[i:i+CHUNK_SIZE] for i in range(0,len(ids),CHUNK_SIZE)):
        js = fetch_json(day, block)
        if js is None:
            log("âŒ  failed block â€“ skip")
            failed_blocks += 1
            if failed_blocks >= MAX_FAILS and not skip_ok:
                sys.exit("ğŸš«  two blocks failed â€“ aborting whole script.")
            continue
        n = upsert(rows_from_json(js))
        log(f"{day} â†’ {n} rows")
    return failed_blocks == 0

# â”€â”€â”€â”€â”€ CLI & Steuerlogik â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--backfill", nargs=2, metavar=("START","END"))
    ap.add_argument("--stations", help="id1,id2,â€¦")
    ap.add_argument("--skip-ok", action="store_true",
                    help="bei Block-Fehlern nur Tag Ã¼berspringen")
    return ap.parse_args()

def main():
    args = parse_args()

    # Datumsbereich bestimmen
    if args.backfill:
        start, end = map(dt.date.fromisoformat, args.backfill)
    else:
        start = end = dt.date.today() - dt.timedelta(days=1)

    # Stationsauswahl
    ids = ALL_IDS
    if args.stations:
        ids = [int(x) for x in args.stations.split(",") if x.strip()]
        missing = set(ids) - set(ALL_IDS)
        if missing: log("âš ï¸  unknown ids ignored:", *missing)

    print(f"â–¶ï¸  Backfill {start} â€¦ {end}  ({(end-start).days+1} days)")

    consec_fail_days = 0
    cur = start
    while cur <= end:
        ok = run_day(cur, ids, args.skip_ok)
        if ok:
            consec_fail_days = 0
        else:
            consec_fail_days += 1
            if consec_fail_days >= MAX_FAILS and not args.skip_ok:
                sys.exit("ğŸš«  two consecutive days failed â€“ abort.")
        cur += dt.timedelta(days=1)

    export_last7()
    if os.path.exists(TEMPLATE):
        with open(TEMPLATE) as r, open(f"{SITE_DIR}/index.html","w") as w: w.write(r.read())
    print("ğŸ‰  Build complete.")

if __name__ == "__main__":
    main()
